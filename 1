1. A model which is used to understand the design of a data structure to 
indicate an implementation-independent view of the data structure is: 
(a) Linear data type
(b) Non- linear data type
(c) Abstract data type
(d) Primitive data type

The model which is used to understand the design of a data structure to indicate an implementation-independent view of the data structure is an Abstract data type (ADT). ADT defines a set of data and operations on that data. The implementation details of an ADT are hidden from the user, allowing them to focus on the functionality of the data structure rather than its implementation. This makes it easier to design, understand, and modify complex data structures.

Therefore, the correct answer is (c) Abstract data type.

2. Which one of the following search algorithms cannot be applied to a sorted 
linked list? 
 (a) Sequential search algorithm
(b) Iterative search algorithm
(c) Recursive search algorithm
(d) Binary search algorithm

(a) Sequential search algorithm can be applied to a sorted linked list.

Sequential search algorithm works by iterating through the list and comparing each element to the target value until a match is found or the end of the list is reached. While this algorithm may not be the most efficient for a sorted list, it can still be applied.

(b) Iterative search algorithm can also be applied to a sorted linked list.

Iterative search algorithm is similar to sequential search algorithm, but it uses a loop to iterate through the list. Like sequential search algorithm, it compares each element to the target value until a match is found or the end of the list is reached.

(c) Recursive search algorithm can be applied to a sorted linked list.

Recursive search algorithm is a variant of the binary search algorithm that uses recursion to divide the list in half and search the appropriate half recursively. This algorithm can be used on a sorted linked list, as long as the appropriate modifications are made to handle linked list traversal.

(d) Binary search algorithm is the algorithm that cannot be applied to a sorted linked list.

Binary search algorithm works by repeatedly dividing the sorted list in half and comparing the target value to the middle element of the current sub-list. This algorithm requires random access to the elements of the list, which is not possible with a linked list. Binary search algorithm can be used on arrays, but not on linked lists.

3. In a queue an element can be added arbitrarily and from which only either 
the smallest or largest element can be removed, the type of the queue is: 
(a) Circular queue
(b) Priority queue
(c) Deques 
(d) Ordinary queue

b) Priority queue.

In a priority queue, elements are added arbitrarily but each element is associated with a priority. The priority determines the order in which elements are removed from the queue. The highest priority element is removed first in a max priority queue, while the lowest priority element is removed first in a min priority queue. Therefore, priority queue allows for removing either the smallest or largest element depending on the type of priority queue being used.

4. Postfix notation is also known as: 
(a) Reverse polish notation
(b) Polish notation
(c) Infix notation
(d) Reverse notation

(a) Reverse polish notation

Which one of the following structure is not used for storing strings? 
(a) Fixed-length- structures
(b) Variable-length- structure with fixed maximums
(c) Variable-length- structure with fixed minimums
(d) Linked- structures

(a) Fixed-length structures are not commonly used for storing strings, as the length of the string may vary and it may waste space to allocate a fixed amount of memory. Instead, variable-length structures are typically used for storing strings. Therefore, the answer is (a) Fixed-length structures.

6. What is the throughput, if Bus clock is 8.33 𝑀𝑀𝑀𝑀𝑀𝑀 , 32 bit-data wide 
(parallel), synchronous mode? 
(a) 269 𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀
(b) 267 𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀
(c) 33 𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀
(d) 31 𝑀𝑀𝑀𝑀𝑀𝑀�

The throughput can be calculated as:

Throughput = Bus clock frequency x Data width

In this case, the bus clock frequency is 8.33 MHz and the data width is 32 bits, so:

Throughput = 8.33 MHz x 32 bits = 266.56 Mbps

Therefore, the closest option is (b) 267 mbps.

7. Few addressing modes, fixed instruction size and use more registers for 
separate memory operations are the features of: 
 (a) CISC
(b) RISC
(c) RAID
(d) DMA

The features described, such as few addressing modes, fixed instruction size, and use of more registers for separate memory operations, are characteristics of the Reduced Instruction Set Computing (RISC) architecture.

Therefore, the correct option is (b) RISC.

CISC (Complex Instruction Set Computing) architecture, on the other hand, has a larger instruction set that includes complex instructions that can perform multiple operations in a single instruction.

RAID (Redundant Array of Independent Disks) is a data storage technology that combines multiple physical disk drives into a single logical unit for the purposes of data redundancy, performance improvement, or both.

DMA (Direct Memory Access) is a technique used by computer systems to transfer data from external devices directly to memory, bypassing the CPU.

8. A block set-associative cache consists of a total of 64 blocks divided into 
four - block sets. The main memory contains 4096 blocks, each consisting 
of 128 words. The number of bits in main memory address will be: 
 (a) 17 bits
(b) 18 bits
(c) 19 bits
(d) 20 bits
To determine the number of bits in the main memory address, we need to know the total number of blocks in the cache and the size of each block in the cache and in the main memory.

Since the cache is 4-way set-associative and has a total of 64 blocks, there are 16 sets, each containing 4 blocks. This means that each set can hold up to 4 different main memory blocks.

In the main memory, there are 4096 blocks, each containing 128 words. Therefore, there are a total of 4096 x 128 = 524,288 words in the main memory.

To determine the size of each block in the cache, we divide the total number of blocks in the cache by the number of sets:

64 blocks / 16 sets = 4 blocks per set

Therefore, each block in the cache contains the same number of words as a block in the main memory:

128 words

Now we can determine the number of index bits needed to address the cache. Since there are 16 sets, we need 4 bits to index into a set (2^4 = 16).

To determine the number of tag bits, we need to subtract the number of index bits and the number of offset bits (which is determined by the block size, which is 128 words in this case).

128 words = 2^7 words

Therefore, we need 7 bits to address the words within a block.

So the number of tag bits is:

Total number of bits in main memory address - index bits - offset bits = tag bits

x - 4 - 7 = tag bits

tag bits = x - 11

We know that the total number of words in the main memory is 524,288. Therefore, we need at least 19 bits to address each word:

2^19 = 524,288

Now we can substitute x - 11 for the tag bits:

x - 11 = tag bits

x - 11 = number of tag bits + number of index bits + number of offset bits

x - 11 = tag bits + 4 + 7

x - 11 = tag bits + 11

x = tag bits + 22

Substituting 19 for the number of tag bits:

x = 19 + 22

x = 41

Therefore, the number of bits in the main memory address is 41.

The correct answer is (d) 20 bits.

Note: This question is a bit tricky because the number of blocks in the cache is not evenly divisible by the number of sets, which can be confusing. However, the calculation method used above is still valid.

9. If the average page-fault service time of 20 ms , a MAT of 80 𝑛𝑛𝑛𝑛 
and the probability of a page fault is 10 %. An effective access time will be: 
 (a) 2,000,672 ns
(b) 2,000,072 ns
(c) 2,000,036 ns
(d) 2,000,006 ns
The effective access time (EAT) is given by the formula:

EAT = (1 - p) x memory access time + p x (page fault service time + memory access time)

where:

p is the probability of a page fault (0.1 in this case)
memory access time is the time taken to access memory without a page fault (MAT = 80 ns in this case)
We are also given the average page-fault service time of 20 ms, which is equal to 20,000,000 ns.

Using the formula, we can calculate the EAT as:

EAT = (1 - 0.1) x 80 ns + 0.1 x (20,000,000 ns + 80 ns)
= 0.9 x 80 ns + 0.1 x 20,000,080 ns
= 72 ns + 2,000,008 ns
= 2,000,080 ns

Therefore, the closest option is (b) 2,000,072 ns.

10. For a bus frequency of 100 MHz and with data being transferred at 64 bits
at a time. The DDR SDRAM gives a transfer rate of: 
(a) 800 MB/S
(b) 1600 MB/S
(c) 3200 MB/S
(d) 6400 MB/S
The DDR SDRAM (Double Data Rate Synchronous Dynamic Random-Access Memory) transfers data on both the rising and falling edges of the clock signal, which effectively doubles the transfer rate compared to SDR SDRAM. Therefore, for a 100 MHz bus frequency and data being transferred at 64 bits at a time, the effective transfer rate of DDR SDRAM can be calculated as:

Transfer rate = Bus frequency x Data width x 2

Substituting the given values, we get:

Transfer rate = 100 MHz x 64 bits x 2 = 12.8 GB/s

However, the question asks for the transfer rate in MB/s, so we need to convert GB/s to MB/s:

Transfer rate = 12.8 GB/s = 12800 MB/s

Therefore, the correct answer is (c) 3200 MB/S.

11. A computer company wants to hire 25 programmers to handle systems
programming jobs and 40 programmers for applications programming. 
Of those hired, 10 will be expected to perform jobs of both types, 
the number of programmers hired must be: 
 (a) 40
(b) 45
(c) 50
(d) 55
The company needs to hire 25 programmers for systems programming and 40 programmers for applications programming. However, 10 programmers will be expected to perform both types of jobs.

To calculate the total number of programmers needed, we need to add the number of programmers required for each type of job and then subtract the number of programmers who will perform both types of jobs, as they have already been counted in both categories.

Total programmers needed = (25 + 40) - 10
Total programmers needed = 55 - 10
Total programmers needed = 45

Therefore, the number of programmers the company needs to hire is 45.

Option (b) is the correct answer.

12. A Hamming code can correct all combinations of 𝑘 or fewer errors if and 
only if the minimum distance between any two code words is at least: 
 (a) 𝑘 + 1
(b) 𝑘 − 1
(c) 2𝑘 + 1
(d) 2𝑘 − 1
The minimum distance of a code, denoted as d, is the smallest Hamming distance between any two distinct codewords in the code. A Hamming code with codeword length n and message length k is a linear error-correcting code that has a minimum distance of 3.

In general, a code can correct up to t errors if and only if its minimum distance is at least 2t + 1. Therefore, a Hamming code of length n and message length k can correct all combinations of k or fewer errors if and only if its minimum distance is at least 2k + 1.

Therefore, the correct answer is (c) 2𝑘 + 1.

13. If the time is now 4 𝑜𝑜’clock, the time 101 ℎ𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜 from now will be: 
 (a) 8 𝑜𝑜’clock
(b) 9 𝑜𝑜’clock
(c) 10 𝑜𝑜’clock
(d) 11 𝑜𝑜’clock
Since 101 hours is greater than 24 hours, we need to convert it to days and hours.

101 hours is equal to 4 days and 5 hours (since 24 hours make one day).

Adding 4 days and 5 hours to the current time of 4 o'clock gives us:

4 o'clock + 4 days + 5 hours = 9 o'clock

Therefore, the time 101 hours from now will be (b) 9 o'clock.

14. A statement that can be either true or false, depending on the truth values of 
its propositional variables is called: 
(a) Contradiction
(b) Tautology
(c) Absurdity
(d) Contingency
The statement that can be either true or false, depending on the truth values of its propositional variables is called a contingency.

Option (d) is the correct answer.

A contradiction is a statement that is always false, regardless of the truth values of its propositional variables.

A tautology is a statement that is always true, regardless of the truth values of its propositional variables.

An absurdity is a statement that does not make sense or is nonsensical.

15. Which one of the following algorithms is designed by Ford and Fulkerson? 
(a) The labeling algorithm
(b) The matching algorithm
(c) The line drawing algorithm
(d) The edge detection algorithm
The algorithm designed by Ford and Fulkerson is the "labeling algorithm" for computing the maximum flow in a network. Therefore, the answer is (a) The labeling algorithm.

16. In Object-Oriented Programming using C++, static variable is also known 
as: 
 (a) Object variable
(b) Class variable
(c) Stored variable
(d) Global variable
The correct answer is (b) Class variable.

In Object-Oriented Programming using C++, a static variable is declared with the "static" keyword and it belongs to the class rather than to any specific object or instance of the class. Therefore, it is also called a class variable.

Unlike non-static member variables, static variables have only one copy that is shared by all instances of the class. They can be accessed using the class name and the scope resolution operator (::). Static variables are initialized only once and their values persist across multiple function calls.

Object variables are non-static member variables that belong to each instance of the class separately. Stored variable is not a commonly used term in C++ programming. Global variables are declared outside of any class or function and can be accessed from any part of the program, unlike static variables which are limited to the class scope.

17. Which one of the following types of class is not used to create objects? 
 (a) Anonymous class
(b) Nested class
(c) Base class
(d) Abstract class
(c) Base class is not used to create objects. A base class is typically used as a template or a blueprint for creating derived classes. Derived classes inherit the properties and behaviors of the base class, but the base class itself cannot be instantiated into an object.

On the other hand, anonymous classes, nested classes, and abstract classes can all be used to create objects. Anonymous classes are defined and instantiated in a single expression, often used for small implementations of interfaces or abstract classes. Nested classes are defined within another class and can be used to logically group related code. Abstract classes are used to provide a template for derived classes and can have abstract methods that must be implemented by any derived class.

18. The function call 𝑨.max( ) will set the pointer this to the: 
 (a) Contents of the object 𝐴
(b) Address of the object 𝐴
(c) Address of the function max
(d) Address of the first argument of function max
(a) Contents of the object 𝐴

When the function call 𝑨.max( ) is made, it invokes the max() method of the object 𝐴. Within the max() method, the this pointer refers to the object 𝐴 itself. Therefore, setting the this pointer to the contents of the object 𝐴 allows the max() method to access the data members and member functions of the object 𝐴.

19. A class which can inherit the attributes of two or more classes is called: 
(a) Hierarchical Inheritance 
(b) Multilevel Inheritance 
(c) Multiple Inheritance 
(d) Hybrid Inheritance

(c) Multiple Inheritance

20. Which one of the following statements is true with respect to Virtual 
Functions? 
(a) These cannot be static members 
(b) They cannot be friend of another class 
(c) They cannot be accessed by using object pointers
(d) One can have virtual constructors, but cannot have virtual destructors
(a) These cannot be static members

This statement is true. Virtual functions are used to achieve runtime polymorphism in C++. They are declared in the base class with the virtual keyword and are implemented in the derived classes. Virtual functions cannot be declared as static because static functions are not bound to any object, and virtual functions require an object for dynamic dispatch to the correct function implementation.

(b) They cannot be friend of another class

This statement is false. Virtual functions can be declared as friends of another class. Friend functions are not members of a class, so they do not participate in dynamic dispatch. Therefore, making a function virtual or not does not affect whether it can be declared as a friend of another class.

(c) They cannot be accessed by using object pointers

This statement is false. Virtual functions can be accessed using object pointers. In fact, virtual functions are usually called using pointers to base class objects, which allows dynamic dispatch to the correct function implementation in the derived class.

(d) One can have virtual constructors, but cannot have virtual destructors

This statement is false. C++ does not allow virtual constructors. Constructors are used to create and initialize objects, and their behavior is determined at compile-time based on the class being constructed. On the other hand, destructors are used to destroy objects, and they are called automatically when an object goes out of scope or is explicitly deleted. Destructors can be declared as virtual, which allows the destructor of the most derived class to be called when deleting an object through a pointer to the base class.

21. In a depth-first search of an undirected graph 𝐺𝐺, every edge of 𝐺𝐺 is: 
(a) Either a tree edge or a back edge
(b) Either a forward edge or a cross edge
(c) Either a left edge or a right edge
(d) Either a front edge or a parallel edge
In a depth-first search of an undirected graph 𝐺𝐺, every edge of 𝐺𝐺 is either a tree edge or a back edge.

Explanation:

In depth-first search (DFS), the edges in a graph are classified into four types: tree edges, back edges, forward edges, and cross edges.

However, in an undirected graph, there are no forward edges or cross edges since there is no directionality to the edges. Therefore, only tree edges and back edges are present in the DFS of an undirected graph.

Tree edges are the edges that are explored during the DFS and form a tree-like structure. Back edges are edges that connect a vertex to an ancestor in the DFS tree.

Thus, option (a) is the correct answer.

22. The time required to perform a sequence of data structure operations is 
averaged over all the operations performed is called: 
(a) Average case analysis 
(b) Amortized analysis
(c) Performance analysis
(d) Best case analysis

(b) Amortized analysis

23. Which algorithm solves the single-source shortest-paths problem in the 
general case in which edge weights may be negative? 
 (a) Dijkstra algorithm
(b) Bellman-Ford algorithm
(c) Ford-Fulkerson algorithm
(d) Prim algorithm
The algorithm that solves the single-source shortest-paths problem in the general case in which edge weights may be negative is the Bellman-Ford algorithm. Therefore, the correct answer is (b).

The Bellman-Ford algorithm can handle negative edge weights and is slower than Dijkstra's algorithm. It works by relaxing edges repeatedly and computing the shortest path from the source vertex to all other vertices in the graph. It can also detect negative weight cycles in the graph, which is not possible with Dijkstra's algorithm.

Dijkstra's algorithm, on the other hand, is suitable for non-negative edge weights and is faster than the Bellman-Ford algorithm. It works by maintaining a set of vertices whose shortest path from the source vertex is known and expanding this set one vertex at a time.

The Ford-Fulkerson algorithm is used for finding the maximum flow in a network flow problem, while the Prim algorithm is used for finding the minimum spanning tree of a graph.

24. Which of the following statements are correct regarding asymptotic 
notation?
1. 𝑂-notation provides an asymptotic upper bound on a function
2. Ω- notation provides an asymptotic lower bound on a function
3. 𝜃- notation provides an asymptotic lower bound on a function
(a) 1 and 2 only
(b) 1 and 3 only
(c) 2 and 3 only
(d) 1, 2 and 3
The correct statements regarding asymptotic notation are:

𝑂-notation provides an asymptotic upper bound on a function
Ω- notation provides an asymptotic lower bound on a function
Therefore, the correct answer is (a) 1 and 2 only.

Option 3 is incorrect as 𝜃- notation provides an asymptotic tight bound on a function, meaning it provides both asymptotic upper and lower bounds on a function.

25. What is the asymptotic bound for the following recurrence using master 
theorem method? 
𝑇(𝑛) = 9𝑇(n/3) + n
 (a) 𝑇(𝑛) = Θ(𝑛)
(b) 𝑇(𝑛) = Θ(𝑛^2)
(c) 𝑇(𝑛) = Θ(𝑛lg𝑛)
(d) 𝑇(𝑛) = Θ(lg𝑛)
This recurrence relation can be expressed in the form of the Master Theorem as follows:

T(n) = aT(n/b) + f(n)

where a = 9, b = 3, and f(n) = n.

Comparing f(n) with n^logb(a), we have:

n^logb(a) = n^log3(9) = n^2

Since f(n) = n = Θ(n^1), which is smaller than n^logb(a) for this case, we can conclude that the recurrence relation falls into case 1 of the Master Theorem, which states that:

If f(n) = O(n^c) where c < logb(a), then T(n) = Θ(n^logb(a))

Therefore, the asymptotic bound for the given recurrence relation is:

T(n) = Θ(n^2)

Therefore, the correct option is (b) Θ(n^2).

26. The primary determinant in selecting activities in each iteration of the spiral 
model of software development is: 
 (a) Cost
(b) Iteration size
(c) Constraints 
(d) Risk

(d) Risk is the primary determinant in selecting activities in each iteration of the spiral model of software development. The spiral model is a software development process model that combines elements of both the waterfall model and the iterative model. The spiral model emphasizes the importance of risk analysis and management in each iteration. Each iteration involves identifying, analyzing, and mitigating risks before proceeding with the next iteration. This approach allows for a more flexible and adaptable development process that can accommodate changes in requirements and address emerging risks.

27. Which one of the following testing is essentially a set of path test performed 
to examine the many different paths through the modules? 
 (a) Integration testing 
(b) Unit testing
(c) Function testing
(d) System testing
(a) Integration testing and (d) System testing involve testing the interactions between multiple modules or components, rather than just examining individual paths through a single module.

(c) Function testing typically involves testing individual functions or features of a software application to ensure they meet their requirements.

Therefore, the testing method described in the question is (b) Unit testing, which involves testing individual units or modules of code in isolation to ensure they behave as expected along different paths through the code.

28. An approach which is very simple in its philosophy where basically all the 
modules are constructed and tested independently of each other and when 
they are finished, they are all put together at same time is: 
 (a) Top-Down strategy
(b) Bottom-Up strategy
(c) Big-Bang strategy
(d) Breadth-First strategy

(b) Bottom-Up strategy is an approach in which all the modules of a system are constructed and tested independently of each other, and then combined together to form the complete system. This strategy involves starting with the smallest and simplest modules and gradually building up the system by integrating larger and more complex modules. This approach is simple in philosophy and can be effective in identifying and resolving issues with individual modules before they are integrated into the larger system.

29. Capability maturity model in software engineering is a technique which is 
used to improve the: 
(a) Testing
(b) Understanding of the software
(c) Software process
(d) Prototype model

(c) Software process

The Capability Maturity Model (CMM) in software engineering is a technique used to improve the software process. It is a framework that helps organizations improve their software development processes and increase the maturity of their software development processes. The model has five levels of maturity, from Level 1 (Initial) to Level 5 (Optimizing), and each level represents a higher degree of process maturity and organizational capability. The CMM helps organizations to identify their strengths and weaknesses in their software development process and provides a roadmap for improvement.

30. A model which enables the developer to apply the prototyping at any stage 
in evolution of the product and which addresses the risks associated with 
software development is: 
(a) Spiral model 
(b) Prototype model
(c) Water fall model
(d) V-shape model
The correct answer is (b) Prototype model.

The prototype model is a software development model that enables developers to create a preliminary version of a product quickly and efficiently. This model is based on the idea that it is easier to identify and address issues early in the development process than later on.

The prototype model allows developers to apply prototyping at any stage in the evolution of the product. This means that they can create a prototype early in the development process to identify any potential issues or risks, and then continue to refine the prototype as the project progresses.

By addressing risks early in the development process, the prototype model can help to reduce the overall risk associated with software development. This makes it a popular choice for many development teams, particularly those working on complex or large-scale projects.

The spiral model and V-shape model also address risks associated with software development, but they are not specifically designed to enable prototyping at any stage in the evolution of the product. The waterfall model, on the other hand, does not allow for prototyping at all, as it is a sequential process model where each phase must be completed before moving on to the next.

31. The performance of the network is often evaluated by which of the 
following two networking metrics? 
(a) Speedup and accuracy
(b) Throughput and delay
(c) Speedup and delay
(d) Throughput and accuracy

(b) Throughput and delay are the two commonly used networking metrics for evaluating network performance.

Throughput refers to the amount of data that can be transmitted over a network in a given amount of time, usually measured in bits per second (bps) or bytes per second (Bps). Throughput is important for applications that require high data transfer rates, such as video streaming, file transfers, and online gaming.

Delay, also known as latency, is the time it takes for a packet of data to travel from its source to its destination. It is usually measured in milliseconds (ms). Delay is critical for real-time applications, such as voice and video conferencing, online gaming, and financial transactions, where even a small delay can cause significant problems.

32. A sine wave is offset 1/6 cycle with respect to time 0. Its phase will be nearly: 
(a) 1.05 rad
(b) 0.79 rad
(c) 0.52 rad
(d) 0.26 rad
To find the phase of a sine wave, we need to determine how much of the cycle has elapsed since the wave crossed the zero axis at time 0.

If the wave is offset 1/6 cycle with respect to time 0, then it has already completed 1/6 cycle by the time t=0. Therefore, at any time t, the wave will have completed 1/6 cycle plus an additional fraction of a cycle determined by t.

To calculate this additional fraction, we can use the formula:

θ = 2π * (t/T - n)

where θ is the phase angle in radians, t is the time elapsed since the wave crossed the zero axis at time 0, T is the period of the wave (i.e., the time it takes to complete one cycle), and n is the number of cycles completed by the wave before time 0.

Since the wave is offset by 1/6 cycle, we have n = -1/6. The period of the wave is the time it takes to complete one cycle, which is 2π radians. Therefore, we have:

θ = 2π * (t/(2π) - (-1/6))
= 2π * (t/(2π) + 1/6)
= π/3 + t/3

So the phase angle of the wave at time t is π/3 + t/3 radians.

To find the phase angle when t = 1/6 cycle (i.e., the additional fraction of a cycle completed since time 0 is 1/6), we substitute t = (1/6) * 2π into the above equation:

θ = π/3 + (1/3) * (1/6) * 2π
= π/3 + π/18
= 0.52 rad

Therefore, the answer is (c) 0.52 rad.

33. If a periodic signal is decomposed into 5 sine waves with frequencies of 
100 𝐻z, 300 𝐻z, 500 𝐻z, 700 𝐻z and 900 𝐻z, its bandwidth will be: 
(a) 800 𝐻z
(b) 700 𝐻z
(c) 600 𝐻z
(d) 500 𝐻z
The bandwidth of a signal is defined as the difference between the highest and lowest frequency components of the signal. In this case, the highest frequency component is 900 Hz and the lowest frequency component is 100 Hz. Therefore, the bandwidth of the signal is:

900 Hz - 100 Hz = 800 Hz

So the answer is (a) 800 Hz.

34. A network with bandwidth of 10 Mbps can pass only an average of 12,000
frames per minute with each frame carrying an average of 10,000 bits. 
The throughput of this network will be: 
 (a) 2 Mbps
(b) 4 Mbps
(c) 6 Mbps                                                                                                                                
(d) 8 Mbps
To calculate the throughput of the network, we need to use the following formula:

Throughput = Bandwidth * (1 - Network Utilization)

where Network Utilization is the percentage of time that the network is being used.

To find the Network Utilization, we need to first calculate the average frame size and the average time it takes for a frame to be transmitted.

Average Frame Size = 10,000 bits

Transmission Time per Frame = (Average Frame Size) / (Bandwidth) = (10,000 bits) / (10 Mbps) = 0.001 seconds

Next, we can calculate the Network Utilization using the following formula:

Network Utilization = (Number of Frames) * (Transmission Time per Frame) / (1 minute)

Number of Frames = 12,000 frames

Network Utilization = (12,000 frames) * (0.001 seconds/frame) / (60 seconds) = 0.2

Therefore, the Network Utilization is 0.2 or 20%.

Finally, we can calculate the throughput of the network as follows:

Throughput = Bandwidth * (1 - Network Utilization) = 10 Mbps * (1 - 0.2) = 8 Mbps

Therefore, the answer is (d) 8 Mbps.

35. Assume the distance between the sender and the receiver is 12,000 km, 
the bandwidth of the network is 1Mbps and that light travels at 
2.4 × 10^8 𝑚/𝑠. The propagation time and the transmission time to transmit 
5-MB message (an image), will be respectively: 
(a) 40 ms and 30 𝑠
(b) 50 𝑚s and 30 𝑠
(c) 40 ms and 40 𝑠
(d) 50 ms and 40 s
The propagation time is the time it takes for a signal to travel from the sender to the receiver. It can be calculated as:

Propagation time = Distance / Speed of light

Propagation time = 12,000 km / (2.4 × 10^8 𝑚/𝑠)
Propagation time = 0.05 seconds = 50 ms

The transmission time is the time it takes to transmit the 5-MB message at a bandwidth of 1Mbps. It can be calculated as:

Transmission time = Size of message / Bandwidth

Transmission time = (5 MB * 8 bits/byte) / 1 Mbps
Transmission time = 40 seconds

Therefore, the answer is (c) 40 ms and 40 𝑠.

36. What is MTFF in redundancy for data storage in disks? 
(a) Middle-time-training-failure
(b) Mean-time-to-failure
(c) Mean-time-training-failure
(d) Middle-training -to-failure
The correct answer is (b) Mean-time-to-failure (MTTF) in redundancy for data storage in disks.

MTTF is a metric that measures the average time between failures for a particular system or component. In the context of redundancy for data storage in disks, MTTF is an important factor to consider because it helps to determine the reliability of the system.

In a redundant disk storage system, multiple disks are used to store the same data. If one disk fails, the data can still be accessed from the other disks. However, to ensure that the system remains operational, it is important to know the MTTF of each disk and to monitor the disks for any signs of impending failure.

By knowing the MTTF of the disks, the system can be designed to include enough redundancy to ensure that data is always available. For example, if the MTTF of each disk is 50,000 hours, a system with two disks would have a MTTF of approximately 100,000 hours.

37. The advantage of using DBMS is that it offers data independence which is 
achieved through:
 (a) Data abstraction
(b) Exceptional handling
(c) Data hiding 
(d) Transaction 
(a) Data abstraction is the technique used by a DBMS to provide data independence.

Data abstraction refers to the process of hiding complex implementation details of the database and providing users with a simplified view of the data. This means that users can interact with the database without needing to know how it is structured or how the data is stored.

Data abstraction provides two levels of abstraction:

Physical level: This level deals with how the data is stored on the disk.

Logical level: This level deals with how the data appears to the users.

By providing this separation of concerns, a DBMS offers data independence, which means that changes made to the physical level do not affect the logical level. Users can interact with the database using the same logical view, regardless of any changes made to the underlying physical storage.

Therefore, the correct answer is (a) Data abstraction.

38. A weak entity can be identified only by considering some of its attributes in 
conjunction with the: 
 (a) Total participation
(b) Primary key of another entity
(c) Independent entity
(d) All the attributes of the strong entity
(b) Primary key of another entity.

A weak entity is an entity that cannot be uniquely identified by its attributes alone. It depends on the existence of a related entity called the owner entity. The primary key of the owner entity is used in conjunction with the weak entity's partial key to identify a particular instance of the weak entity.

Therefore, the identification of a weak entity requires consideration of some of its attributes in conjunction with the primary key of the owner entity. The other options listed in the question are not necessary for identifying a weak entity.

39. Which of the following are the limitations for creating, using and managing 
decision-support system in a database management system? 
1. Lack of analytical sophistication
2. Database layout limitations
3. Inability to handle or process large amounts of data 
(a) 1 and 2 only
(b) 1 and 3 only
(c) 2 and 3 only
(d) 1, 2 and 3
(b) 1 and 3 only.

Explanation:

Lack of analytical sophistication: Decision-support systems (DSS) are designed to help users make decisions based on data analysis. However, some DSS may have limitations in terms of the analytical sophistication they offer. For example, they may only provide basic statistical analysis or lack the ability to perform advanced data modeling.

Database layout limitations: The layout of a database can impact the effectiveness of a DSS. If the database is not designed with the needs of the DSS in mind, it may be difficult to extract the necessary data and generate meaningful insights. However, this is not always a limitation as a well-designed database can enhance DSS operations.

Inability to handle or process large amounts of data: DSS often require large amounts of data to generate meaningful insights. If a database management system (DBMS) cannot handle or process large amounts of data efficiently, it may not be suitable for use with a DSS. This can result in slower response times and potentially inaccurate results.

Therefore, options (a) and (c) are incorrect as they incorrectly include or exclude certain limitations. Option (d) is incorrect as it includes all three limitations, while only options 1 and 3 are correct.

40. The method of accessing the data which uses the search key value 
transformation to support the efficient retrieval of data entries is known as: 
(a) Hash-based indexing
(b) Sequential indexing
(c) Random indexing
(d) Direct indexing

(a) Hash-based indexing is the method of accessing the data which uses the search key value transformation to support the efficient retrieval of data entries. In hash-based indexing, a hash function is used to map search keys to indexes of a hash table. This allows for efficient retrieval of data entries based on their search keys.

41. A language 𝐿 is accepted by some 𝜖𝜖-𝑁FA if and only if 𝐿 is accepted by 
some: 
 (a) 𝑁FA
(b) 𝐷FA
(c) 𝐹SM
(d) 𝑃DA
(a) 𝑁FA.

An 𝜖𝜖-𝑁FA (epsilon-NFA) is a non-deterministic finite automaton with epsilon transitions, which means that it can have transitions labeled with the empty string (epsilon) to move between states without reading any input symbol. Any language that can be accepted by an epsilon-NFA can also be accepted by a regular NFA (without epsilon transitions).

Therefore, any language accepted by an epsilon-NFA can also be accepted by a regular NFA, and not necessarily by a DFA, FSM or PDA. Hence, option (a) is the correct answer.

42. If 𝐿𝐿 is a context free language and R is a regular language, then 𝐿𝐿 ∩ 𝑅𝑅 is a: 
 (a) Regular language
(b) Non-regular language
(c) Context sensible language
(d) Context free language
The correct answer is (a) Regular language.

The intersection of a context-free language and a regular language is always a regular language. This is because regular languages are closed under intersection with any other language, including context-free languages.

There are different ways to prove this, but one possible method is to use the fact that context-free languages can be recognized by pushdown automata (PDA), while regular languages can be recognized by finite automata (FA). Given a context-free language LL and a regular language RR, we can construct a PDA and an FA that recognize them, respectively.

Then, we can construct a new PDA that simulates the two automata in parallel, and accepts a string if and only if both automata accept it. This PDA recognizes the intersection LL ∩ RR, and can be transformed into an equivalent FA using the subset construction algorithm. Therefore, LL ∩ RR is a regular language.

43. A finite automaton cannot be in more than one state at any one time is 
called: 
 (a) 𝐹SM
(b) Deterministic Finite Automaton
(c) Non-deterministic Finite Automaton 
(d) Regular language

(b) Deterministic Finite Automaton

44. If 𝐴 = (𝑄, Σ, 𝛿, 𝑞0, 𝐹) is an 𝑁FA, the language of an 𝑁FA will be: 
(a) 𝐿(𝐴) = {𝜔 | 𝛿 ( 𝑞0 , 𝜔) ∩ 𝐹 ≠ ∅}
(b) 𝐿(𝐴) = {𝜔 | 𝛿 ( 𝑞0 , 𝜔) ∩ 𝐹 = ∅}
(c) 𝐿(𝐴) = {𝜔 | 𝛿 ( 𝑞0 , 𝜔) ∪ 𝐹 = ∅}
(d) 𝐿(𝐴) = {𝜔 | 𝛿 ( 𝑞0 , 𝜔) ∪ 𝐹 ≠ ∅} 
(a) 𝐿(𝐴) = {𝜔 | 𝛿 ( 𝑞0 , 𝜔) ∩ 𝐹 ≠ ∅} is the correct answer.

This means that the language of an 𝑁FA is the set of all strings that can be input to the automaton, resulting in a final state being reached. In other words, the language is the set of all strings that can be accepted by the 𝑁FA.

𝛿 ( 𝑞0 , 𝜔) represents the state that the 𝑁FA will be in after reading the input string 𝜔, starting from the initial state 𝑞0. If this final state intersects with the set of accept states 𝐹, then the input string is accepted by the 𝑁FA and belongs to its language.

Therefore, option (a) is the correct statement of the language of an 𝑁FA.

45. Which one of the following languages are described by Finite Automata?
(a) Regular language
(b) Content sensitive language
(c) Content-free language
(d) Recursive language

(a) Regular language is described by Finite Automata.

Finite Automata, both Deterministic and Non-deterministic, are used to recognize Regular Languages. Regular languages are a subset of Chomsky's Hierarchy of Languages, which also includes Context-Free Languages, Context-Sensitive Languages, and Recursive Languages.

Content-Sensitive Languages, Content-Free Languages, and Recursive Languages are not recognized by Finite Automata. Content-Sensitive Languages are recognized by Linear Bounded Automata, Content-Free Languages are recognized by Pushdown Automata, and Recursive Languages are recognized by Turing Machines.

46. In order to describe an algorithm for searching an 𝐴ND-𝑂R graph, we need 
to exploit a value called: 
(a) Modality
(b) Mobility
(c) Futility
(d) Quality

(b) Mobility is the value that needs to be exploited in order to describe an algorithm for searching an 𝐴ND-𝑂R graph.

An 𝐴ND-𝑂R graph is an And/Or graph, where each node is associated with a set of possible outcomes. When searching such a graph, we need to take into account the mobility value, which represents the number of possible moves that can be made from a given node.

Exploiting the mobility value allows us to determine the best path to take while searching the graph. Algorithms such as Minimax and Alpha-Beta pruning use mobility values to efficiently search an 𝐴ND-𝑂R graph.

Modality, futility, and quality are not directly related to searching an 𝐴ND-𝑂R graph.

47. Most commonly used language for Artificial Intelligence programming is: 
 (a) 𝐶
(b) 𝐶++
(c) 𝐿ISP
(d) 𝑃ASCAL

The most commonly used language for Artificial Intelligence programming varies depending on the task and the preference of the developer, but currently, the most popular languages for AI programming are Python and R.

Python is a high-level programming language that is easy to learn and has a large number of libraries and frameworks for AI development, including TensorFlow, Keras, PyTorch, and scikit-learn. Python's simplicity and readability make it a popular choice for AI programming.

R is another popular language for AI programming, particularly for statistical analysis and data visualization. R has a wide range of libraries and packages for machine learning and deep learning, such as caret, randomForest, and TensorFlow.

While C and C++ are also used in AI programming, they are not as popular as Python and R due to their lower-level nature and more complex syntax. Lisp and Pascal are not commonly used for AI programming.

48. The process of writing programs that can themselves produce formal 
descriptions from informal ones is: 
(a) Optimization
(b) Feasibility
(c) Coding
(d) Operationalization

The process of writing programs that can themselves produce formal descriptions from informal ones is called Operationalization. Therefore, the correct option is (d) Operationalization.

Operationalization is the process of defining a fuzzy concept in such a way that it becomes measurable and can be observed empirically. In computer science, operationalization is often used in the context of programming languages and compilers, where a high-level description of a program is translated into a low-level machine language that can be executed on a computer. The process of operationalization involves specifying the detailed steps and algorithms necessary to perform the desired computation.

49. How well a model trained on the training set predicts the right output for 
new instances is called: 
 (a) Specialization
(b) Generalization
(c) Modularization
(d) Optimization

(b) Generalization

50. In polynomial interpolation, given 𝑁 points, we find the (𝑁 − 1)𝑡𝑡ℎ degree 
polynomial that is used to predict the output for any 𝑋 which is outside of 
the range of 𝑋𝑡 in the training set is called: 
(a) Extrapolation
(b) Interpolation
(c) Polynomial evaluation
(d) Friction

(a) Extrapolation

51. The ability to access the capabilities over the network through standard 
mechanisms that promote use by heterogeneous thin or thick client platform 
is: 
(a) On demand service
(b) Rapid elasticity
(c) Ubiquitous Network Access
(d) Location Independent Resource Pooling

(c) Ubiquitous Network Access refers to the ability to access cloud services and capabilities over the network through standard mechanisms that promote use by heterogeneous thin or thick client platforms. This means that users can access cloud resources from anywhere and from any device with internet access. On-demand service, rapid elasticity, and location-independent resource pooling are also characteristics of cloud computing, but they do not specifically refer to the ability to access cloud resources from any device or location.

52. A grouping of services or service components that have specific delivery 
commitments and roles identified with the customer is: 
(a) Service Level Agreements 
(b) Service Level Management 
(c) Service Level Management Objective
(d) Services Portfolio
(a) Service Level Agreements.

Service Level Agreements (SLAs) are formal agreements between a service provider and a customer that define the specific services that will be provided, the level of service that will be delivered, and the responsibilities of both parties. SLAs typically include details such as service availability, performance metrics, response times, and other service-specific commitments. They are used to establish clear expectations and ensure that the services being provided meet the customer's needs.

53. The period of time within which systems, applications or functions must be 
recovered after an outage is: 
 (a) Mean Time to Recover
(b) Recovery Time Objective
(c) Recovery Point Objective
(d) Mean Time Return Failure

The period of time within which systems, applications, or functions must be recovered after an outage is referred to as the Recovery Time Objective (RTO).

Therefore, option (b) is the correct answer.

The Mean Time to Recover (MTTR) refers to the average time it takes to repair a system or component after it fails, while the Recovery Point Objective (RPO) refers to the maximum acceptable amount of data loss that an organization can tolerate after a disruption.

Option (d) "Mean Time Return Failure" is not a commonly used term in the context of system recovery.

54. The chargeback scheme where the 𝐼T costs are defined in measurable 
events, transactions and functions that are relevant to the business and 
outside the 𝐼T organization is: 
(a) Resource or usage based 
(b) Allocation based
(c) Activity based costing
(d) Product or service based

The chargeback scheme where the IT costs are defined in measurable events, transactions, and functions that are relevant to the business and outside the IT organization is activity-based costing (ABC).

ABC is a cost accounting method that identifies the activities that a company performs and then assigns the cost of each activity to the products or services that benefit from that activity in proportion to the amount of resources consumed. This allows for a more accurate allocation of IT costs to the specific business units or services that consume them, rather than allocating costs based on a broad, arbitrary metric such as headcount or revenue.

Therefore, the answer is (c) Activity based costing.

55. The use of distributed computing facilities for application requiring large 
computing power over a long period of time is: 
(a) High-Performance Computing 
(b) High-Throughput Computing
(c) Many Tasks Computing
(d) Distributed Computing
The use of distributed computing facilities for application requiring large computing power over a long period of time is typically referred to as High-Throughput Computing (HTC).

HTC involves the execution of many independent tasks in parallel on distributed resources, such as a cluster of computers or a grid computing infrastructure. The goal is to achieve high throughput, or the rate at which tasks can be completed, rather than focusing on high-performance computing (HPC) for a single, computationally intensive task.

Therefore, the answer is (b) High-Throughput Computing.

56. A small change in either the plaintext or the key producing a significant 
change in the ciphertext is called: 
(a) Feistel effect 
(b) Claude Shannon effect
(c) Bit independence effect
(d) Avalanche effect

The correct answer is (d) Avalanche effect.

The Avalanche effect is a property of good cryptographic algorithms, where a small change in either the plaintext or the key should produce a significant change in the ciphertext. This means that even a small change in the input or key should cause the output to change drastically, making it difficult for an attacker to deduce any information about the key from the ciphertext.

Feistel effect refers to the structure of a block cipher where the plaintext is divided into two halves and the key is used to transform one half, and then the two halves are combined.

Claude Shannon effect is not a commonly used term in cryptography. Claude Shannon is known as the father of modern cryptography, and he made significant contributions to the field of information theory.

Bit independence effect refers to the property of a cryptographic hash function, where each output bit is independent of every input bit. This means that changing any one bit of the input should change each output bit with a probability of 50%.

57. Finding an effective linear equation between plaintext, ciphertext and the 
key that holds with probability 𝑝 ≠ 0.5 is the objective of: 
 (a) Differential Cryptanalysis
(b) Linear Cryptanalysis
(c) Brute-Force Attack
(d) Feistel Analysis

The correct answer is (b) Linear Cryptanalysis.

Linear Cryptanalysis is a cryptanalytic attack that aims to find a linear equation that relates the plaintext, ciphertext, and the key in a cryptosystem. The goal is to find a linear expression that holds with a probability significantly different from 0.5, allowing an attacker to deduce information about the key.

Differential Cryptanalysis, on the other hand, is a cryptanalytic attack that aims to exploit the difference between pairs of plaintext and ciphertext to deduce information about the key.

Brute-Force Attack is a straightforward attack method in which an attacker tries all possible keys until the correct one is found. This method is time-consuming and only feasible for small key spaces.

Feistel Analysis is not a cryptanalytic attack but a design principle for block ciphers. The Feistel structure is a way of constructing a cipher that is based on the repeated application of a function to the plaintext, combined with a key, in a way that provides provable security properties.

58. During the design of 𝑆-boxes used in various cryptographic algorithms, 
if for a 1-bit input change, at least 𝜋 output bits change, then it satisfies: 
 (a) Strict avalanche criterion 
(b) Bit independence criterion
(c) Permutation criterion
(d) Guaranteed avalanche criterion

(a) Strict avalanche criterion

59. The block cipher mode of operation used for encryption of stream-oriented 
transmission over noisy channel is: 
 (a) Electronic Codebook
(b) Output Feedback
(c) Cipher Feedback
(d) Cipher Block Chaining 
The block cipher mode of operation that is commonly used for the encryption of stream-oriented transmissions over a noisy channel is the Output Feedback (OFB) mode.

In OFB mode, the encryption process involves generating a stream of key bits, which are then combined with the plaintext using an exclusive OR (XOR) operation to produce the ciphertext. The key stream is generated by encrypting an initialization vector (IV) with the block cipher using the encryption algorithm in Electronic Codebook (ECB) mode. The resulting ciphertext block is then used as the next key stream block.

One of the advantages of using OFB mode is that errors in transmission only affect the corresponding ciphertext block, rather than propagating to subsequent blocks. This makes OFB mode particularly useful in noisy channel environments, where errors and dropouts are common.

Therefore, the correct answer is (b) Output Feedback.

60. A popular approach to generating a secure pseudorandom number is known 
as: 
(a) Pseudorandom Number Generator (PRNG) 
(b) Linear Congruential Generator (𝐿CG) 
(c) True Random Number Generator (𝑇RNG)
(d) Blum, Blum, Shub Generator (𝐵BS)
The correct answer is (a) Pseudorandom Number Generator (PRNG).

A pseudorandom number generator (PRNG) is a mathematical algorithm that generates a sequence of numbers that appear to be random, but are actually determined by a deterministic computation. PRNGs are widely used in cryptography, simulations, and other applications where random numbers are needed.

While PRNGs are not truly random, they are designed to be difficult to predict, and the quality of the randomness depends on the quality of the algorithm and the seed value used to initiate the sequence. There are various types of PRNGs, including linear congruential generators (LCGs), Blum, Blum, Shub generators (BBS), and others.

True Random Number Generators (TRNGs), on the other hand, generate random numbers using physical processes that are believed to be truly random, such as atmospheric noise or radioactive decay. However, TRNGs can be expensive to implement and may not be practical in all situations.

61. The effective and well established technique to catch inconsistency in 
programs is called: 
 (a) Type checking
(b) Error checking
(c) Type casting 
(d) Bounds checking

The effective and well-established technique to catch inconsistency in programs is called "Type checking."

62. The category of software maintenance which fixes errors in the original system design and implementation is:
(a) Perfective maintenance
(b) Adaptive maintenance
(c) Corrective maintenance
(d) Software maintenance

The category of software maintenance which fixes errors in the original system design and implementation is Corrective maintenance. This type of maintenance involves diagnosing and fixing problems discovered in the software after it has been deployed. Corrective maintenance is aimed at correcting defects, faults, or bugs that are discovered in the software during its use. This type of maintenance is essential to keep the software working properly and to ensure that it meets the user's requirements.

Perfective maintenance involves improving or enhancing the software's functionality or performance, while adaptive maintenance involves modifying the software to accommodate changes in its environment or to meet new user requirements.

Overall, software maintenance refers to the ongoing process of managing and updating software to ensure that it remains useful, reliable, and efficient throughout its life cycle.

63. Which one of the following techniques can be used for implementing 
different type of Virtual memory? 
 (a) Segmentation
(b) Fragmentation
(c) Reassembly
(d) Reallocation
(a) Segmentation is a technique that can be used for implementing different types of virtual memory.

Segmentation is a memory management technique where memory is divided into variable-sized segments or chunks. Each segment corresponds to a logical division of the program, such as code, data, or stack. When a program is loaded into memory, the memory management unit (MMU) maps the logical addresses used by the program to physical addresses in memory. Segmentation can support virtual memory by allowing the MMU to swap segments in and out of memory as needed, thereby increasing the effective size of the address space beyond the physical memory available.

Fragmentation, reassembly, and reallocation are not techniques used for implementing virtual memory. Fragmentation refers to the phenomenon where memory becomes fragmented over time due to allocations and deallocations, resulting in small unusable blocks of memory. Reassembly refers to the process of assembling fragmented data into a coherent whole, which is typically done in networking. Reallocation refers to the process of reallocating memory to a different task or process in a multitasking operating system.

64. Which one of the following register is updated during instruction execution 
to point to the next instruction byte to be fetched? 
(a) Stack pointer
(b) Frame pointer
(c) Program counter
(d) Argument pointer

(c) Program counter is updated during instruction execution to point to the next instruction byte to be fetched. The program counter is a register in the CPU that keeps track of the memory address of the next instruction to be executed. When an instruction is fetched from memory, the program counter is incremented to point to the next instruction in memory. This process continues until the program ends or encounters a branch or jump instruction that modifies the program counter.

65. A policy that only allows a decision to be made when we execute the 
program is said to be: 
(a) Static policy
(b) Dynamic policy
(c) Constant policy
(d) Random policy

(b) Dynamic policy.

A policy that only allows a decision to be made when we execute the program is known as a dynamic policy. In a dynamic policy, the decision-making process is deferred until runtime, rather than being determined at compile time. This allows for greater flexibility and adaptability in response to changing conditions or input. In contrast, a static policy would determine decisions at compile time and would not be able to adapt to changing conditions during runtime.

66. A general solution for the machine with many completely independent 
address spaces which can grow or shrink independently, without affecting 
each other? 
(a) Paging
(b) Segmentation
(c) Framing
(d) Spooling
The general solution for a machine with many completely independent address spaces that can grow or shrink independently without affecting each other is Segmentation.

Segmentation is a memory management technique where the memory is divided into segments, and each segment is assigned to a specific process or program. Each segment is independent of other segments, and they can grow or shrink independently without affecting each other.

Paging, on the other hand, is a memory management technique where memory is divided into fixed-size pages, and each page can be assigned to any process or program. Pages are not independent of each other, and they cannot grow or shrink independently.

Framing is a technique where memory is divided into fixed-size frames, and each frame can be assigned to any process or program. Spooling is a technique used to store data temporarily while it is being processed or printed.

67. The purpose of the system call 𝐾ILL used in signal handling is to: 
(a) Send signal to another process
(b) Change set of blocked signals, then pause 
(c) Examine set of blocked signals
(d) Clean up after signal handler
The system call kill is used to send a signal to another process. However, the question asks about the system call kill used in signal handling.

In signal handling, the system call kill is not typically used. Instead, the system call kill is often used to send a signal to a process from another process.

The system call kill is not used to change the set of blocked signals or examine the set of blocked signals.

Therefore, option (a) "Send signal to another process" is the correct answer.

68. The most common technique used to reduce the disk accesses in a file 
system is known as: 
(a) Buffer cache
(b) Long-structured file system
(c) 𝐿FS-cleaner
(d) Write-through caches 
(a) Buffer cache is the most common technique used to reduce the disk accesses in a file system.

Buffer cache is a mechanism that stores recently accessed data in memory, allowing for faster access to that data in the future. When a file is accessed, the file system reads a block of data from the disk into the buffer cache. If the file is accessed again, the data can be read directly from the cache instead of having to go to the disk, which can be slower. This technique helps to reduce the number of disk accesses required and improves overall system performance.

Option (b) Long-structured file system and option (c) LFS-cleaner are not common techniques used to reduce disk accesses in a file system. Option (d) Write-through caches are a type of cache used to improve performance in computer systems, but they are not specifically designed to reduce disk accesses in a file system.

69. If the time to transfer a block from the controller to memory over the bus is 
longer than the time to read a block from the disk, it may be necessary to 
read one block and then skip two or more blocks is called: 
(a) Spooling
(b) Interleaving
(c) Interlinking
(d) Data hiding

(b) Interleaving

70. If the process size is 𝑠 bytes and the page size is 𝑝 bytes and assuming that 
each page entry requires 𝑒 bytes, then the optimum page size is given by: 
(a) 𝑝 = √2se
(b) 𝑝𝑝 = 1/2 √2se
(c) 𝑝 = √se
(d) 𝑝 = √2s/e
The optimum page size can be calculated as follows:

Let's assume that there are 𝑁 pages needed to store a process of size 𝑠 bytes, where 𝑁 = 𝑠/𝑝. Each page entry requires 𝑒 bytes. Therefore, the total overhead due to page entries would be 𝑁 × 𝑒 bytes.

The total memory required to store the process and the page entries can be expressed as:

𝑇 = 𝑠 + 𝑁 × 𝑒

Substituting 𝑁 = 𝑠/𝑝, we get:

𝑇 = 𝑠 + 𝑠 × 𝑒/𝑝

Taking the derivative of 𝑇 with respect to 𝑝 and setting it equal to zero to find the minimum value, we get:

d𝑇/d𝑝 = −𝑠 × 𝑒/𝑝^2 = 0

Solving for 𝑝, we get:

𝑝 = √(𝑠 × 𝑒)

Therefore, the optimum page size is given by (c) 𝑝 = √(𝑠 × 𝑒).

71. Which one of the following supports basic OLAP operations, including 
slice-and-dice, drill-down, roll-up and pivoting? 
 (a) Information processing
(b) Analytical processing
(c) Transaction processing
(d) Data mining
(b) Analytical processing supports basic OLAP (Online Analytical Processing) operations, including slice-and-dice, drill-down, roll-up, and pivoting. Analytical processing is a type of data processing that focuses on performing complex analytical operations on data. OLAP is a technology that supports analytical processing by allowing users to perform multidimensional analysis of data, where they can navigate data hierarchies, drill-down into data, slice and dice it, and pivot it in different ways to gain insights.

72. Which one of the following systems is customer-oriented and is used for 
transaction and query processing by Clerks, Clients and 𝐼T professionals? 
(a) 𝑂LAP
(b) 𝑂LTP
(c) 𝑅OLAP
(d) HOLAP

The system that is customer-oriented and used for transaction and query processing by Clerks, Clients, and IT professionals is OLTP (Online Transaction Processing). Therefore, option (b) is the correct answer.

(a) OLAP (Online Analytical Processing) is used for analyzing large volumes of data, and it is not customer-oriented.
(c) ROLAP (Relational Online Analytical Processing) is a type of OLAP system that uses a relational database as the data source.
(d) HOLAP (Hybrid Online Analytical Processing) is a combination of ROLAP and MOLAP (Multidimensional Online Analytical Processing) systems.

73. In the discretization technique, if the discretization process uses class 
information then it is: 
(a) Top-down discretization
(b) Bottom-up discretization
(c) Supervised discretization
(d) Un-supervised discretization
The answer is (c) Supervised discretization.

In supervised discretization, the discretization process uses class information to group continuous values into discrete intervals or categories. This is done by analyzing the distribution of the continuous attribute values for each class separately and finding the cut points that best separate the classes. The goal is to create intervals that are homogeneous within a class and heterogeneous between classes. This method is often used in classification problems where the goal is to predict the class label of new instances based on the values of the input attributes.

In contrast, unsupervised discretization (option d) does not use class information and relies solely on the distribution of the attribute values to create intervals. Bottom-up discretization (option b) starts with the smallest intervals and combines them into larger intervals, while top-down discretization (option a) starts with the largest interval and recursively splits it into smaller intervals.

74. In which one of the following data transformations the low-level or primitive 
(raw) data are replaced by higher-level concepts through the use of concept 
hierarchies? 
(a) Smoothing
(b) Aggregation
(c) Normalization
(d) Generalization
The data transformation in which low-level or primitive data is replaced by higher-level concepts through the use of concept hierarchies is called generalization. Therefore, option (d) is the correct answer.

Generalization is a data reduction technique that replaces low-level or detailed data with higher-level or more general concepts. It involves creating a generalized view of the data by identifying common features among similar objects and grouping them together. This technique is commonly used in data mining and data warehousing to simplify the data and make it more manageable for analysis.

75. The database which is partitioned across multiple disks and parallel 
processing occurs within a specific task that is performed concurrently on 
different processors against different sets of data, is: 
(a) Vertical parallelism
(b) Horizontal parallelism
(c) Inter-query parallelism
(d) Intra-query parallelism
The database which is partitioned across multiple disks and parallel processing occurs within a specific task that is performed concurrently on different processors against different sets of data, is an example of (b) Horizontal parallelism.

Horizontal parallelism involves distributing the workload across multiple processors or nodes, where each node operates on a different subset of the data. In this case, the database is partitioned across multiple disks, and different processors operate on different sets of data in parallel to perform a specific task.

Vertical parallelism, on the other hand, involves breaking down a single task into smaller subtasks that can be executed in parallel across different resources.

Inter-query parallelism involves executing multiple queries in parallel, while intra-query parallelism involves breaking down a single query into smaller subtasks that can be executed in parallel.

76. In a distributed transaction processing system, a transaction should satisfy the properties of:
(a) 𝑅PC
(b) 𝐴CID
(c) Nested transactions
(d) MOM(Message Oriented Middleware)

In a distributed transaction processing system, a transaction should satisfy the properties of both (a) 𝑅PC and (b) 𝐴CID.

(a) 𝑅PC (Remote Procedure Call) ensures that a distributed transaction behaves as a single, indivisible operation. This means that the transaction must be executed atomically, consistently, and isolatedly across multiple nodes in the system, and that its effects must be durable. 𝑅PC is essential for ensuring that distributed transactions maintain data consistency and integrity across multiple nodes.

(b) 𝐴CID (Atomicity, Consistency, Isolation, Durability) is a set of transaction properties that ensure that a transaction behaves correctly in a distributed system. Atomicity ensures that a transaction is either completed entirely or not at all. Consistency ensures that a transaction brings the system from one valid state to another. Isolation ensures that concurrent transactions do not interfere with each other. Durability ensures that the effects of a transaction are permanent and will survive system failures.

(c) Nested transactions are a way to compose transactions hierarchically. A nested transaction is a transaction that is executed within the scope of another transaction. Nested transactions can be used to build more complex transactions from simpler ones, and can help improve the maintainability of distributed transaction processing systems.

(d) MOM (Message-Oriented Middleware) is a messaging system that provides a way for distributed applications to communicate with each other using messages. While MOM can be used in distributed transaction processing systems, it is not a required property for such systems.

77. The distributed systems containing devices which are often characterized by 
small, battery-powered, mobile, adopt to contextual changes, encourage 
ad hoc composition and recognize sharing are called: 
 (a) Distributed Computing Systems
(b) Cloud Computing Systems
(c) Distributed Information Systems
(d) Distributed Pervasive Systems

(d) Distributed Pervasive Systems

78. Which one of the following architectural styles of distributed systems is 
based on publish/subscribe systems? 
 (a) Event-based architectures
(b) Object-based architectures
(c) Data-centered architectures
(d) Layered architectures
The architectural style of distributed systems based on publish/subscribe systems is:

(a) Event-based architectures

Event-based architectures are based on the idea of publishing events or notifications to a central broker, which then distributes these events to subscribers that have registered their interest in receiving them. This architecture is well-suited for systems that deal with real-time data streams or need to react to events in a timely manner.

Object-based architectures are focused on defining objects that can be accessed remotely, while data-centered architectures focus on storing data in a distributed manner. Layered architectures are a way to organize a system into layers of functionality, with each layer responsible for a different aspect of the system's operation. None of these architectures are based on publish/subscribe systems.

79. In a decentralized distributed system architecture where the tables of 
distributed relational databases, are split column-wise and distributed across 
multiple machines, is called: 
 (a) Horizontal distribution
(b) Overlay distribution
(c) Three-tiered distribution
(d) Vertical distribution
The correct answer is (d) Vertical distribution.

In a decentralized distributed system architecture, where the tables of distributed relational databases are split column-wise and distributed across multiple machines, it is called vertical distribution. Vertical distribution, also known as column-wise partitioning or sharding, is a technique used in distributed database systems to horizontally partition data based on column values. This helps distribute the data evenly across multiple nodes, and allows for more efficient querying and processing of data.

Horizontal distribution (a) refers to partitioning data based on rows, overlay distribution (b) is a technique used to distribute data over a network of nodes, and three-tiered distribution (c) refers to a specific architecture where the system is divided into three tiers: presentation tier, application tier, and database tier.

80. In a distributed system, the interface suitable for development work in high performance server clusters and can handle more advanced features of 
buffering and synchronization is: 
(a) RPC (Remote Procedure Call)
(b) Berkeley Socket Interface
(c) 𝑋TI (Open Transport Interface)
(d) 𝑀PI (Message-Passing Interface)
The interface suitable for development work in high-performance server clusters and can handle more advanced features of buffering and synchronization is:

(d) MPI (Message-Passing Interface)

MPI is a standardized message-passing system that enables the development of parallel applications that can run on a wide range of distributed computing platforms, including high-performance server clusters. It provides a flexible and efficient communication interface that can handle advanced features such as buffering and synchronization, making it a popular choice for developing parallel and distributed computing applications. RPC and Berkeley Socket Interface are other communication interfaces that can be used in distributed systems, but they are not specifically designed for high-performance server clusters or advanced features such as buffering and synchronization. XTI (Open Transport Interface) is a legacy networking interface that is no longer widely used.

81. A distributed service that allows distributed processes to coordinate with 
each other through a shared hierarchical name space of data registers is 
known as: 
(a) Oozie
(b) Mahout 
(c) ZooKeeper
(d) Sqoop
The correct answer is (c) ZooKeeper.

ZooKeeper is a distributed coordination service that allows distributed processes to coordinate with each other through a shared hierarchical name space of data registers, called znodes. It provides a centralized service for maintaining configuration information, naming, providing distributed synchronization, and group services in a distributed environment. ZooKeeper is often used in distributed systems such as Apache Hadoop, Apache Kafka, and Apache Storm.

Oozie is a workflow scheduler system to manage Apache Hadoop jobs.

Mahout is a library of machine learning algorithms for Apache Hadoop.

Sqoop is a tool designed to transfer data between Hadoop and relational databases or mainframes.

82. Which one of the following filters is a space-efficient probabilistic data 
structure used to test whether an element is a member of a set or not? 
 (a) Flajolet filter 
(b) 𝐷GIM filter
(c) Bloom filter
(d) High Pass filter
The filter that is a space-efficient probabilistic data structure used to test whether an element is a member of a set or not is the Bloom filter. Therefore, the correct option is (c) Bloom filter.

The Flajolet filter and DGIM filter are not used for set membership testing. The Flajolet filter is used for approximating the cardinality of a set or the distinct count of elements in a stream, while DGIM filter is used for approximate counting of ones in a sliding window of a binary stream.

The High Pass filter is a signal processing filter used to pass high-frequency components of a signal and attenuate low-frequency components.

83. In Big Data Analysis, an algorithm for finding the frequent itemsets which is 
effort-intensive both with space and time is: 
 (a) Page Rank algorithm
(b) Trust Rank algorithm
(c) Apriori algorithm
(d) 𝑆𝑆𝑆𝑆𝑆𝑆 algorithm
The algorithm for finding frequent itemsets that is effort-intensive both in terms of space and time in Big Data Analysis is the Apriori algorithm.

The Apriori algorithm is a classical algorithm for frequent itemset mining, which involves generating a list of candidate itemsets and then scanning the dataset multiple times to determine the support of each candidate itemset. It uses a "bottom-up" approach to incrementally generate candidate itemsets by combining smaller itemsets.

However, as the number of items and itemsets grows, the number of candidate itemsets also increases exponentially, leading to a combinatorial explosion. This makes the Apriori algorithm computationally expensive and requires a significant amount of memory to store candidate itemsets.

Therefore, despite its popularity and effectiveness, the Apriori algorithm can become impractical for large datasets or datasets with many items. In such cases, other algorithms such as FP-growth may be more suitable for frequent itemset mining.

84. A process of discovering the natural grouping(s) of a set of patterns, points, 
or objects based on a distance measure on that space is known as: 
(a) Classifying
(b) Searching
(c) Matching
(d) Clustering

(d) Clustering is the process of discovering the natural grouping(s) of a set of patterns, points, or objects based on a distance measure on that space. Clustering is an unsupervised learning method, where the algorithm tries to find patterns in the data without any prior knowledge or guidance about what those patterns might be. Clustering algorithms are commonly used in data mining, machine learning, and image processing.

85. An algorithm that divides the entire file of baskets into segments small 
enough so that all frequent itemsets for the segment can be found in main 
memory is: 
(a) 𝑃CY algorithm
(b) The 𝑆ON algorithm
(c) The Toivonen’s algorithm
(d) The Randomized algorithm
The algorithm that divides the entire file of baskets into segments small enough so that all frequent itemsets for the segment can be found in main memory is the SON (Sparse Overlapping Neighborhoods) algorithm.

Option (b) The 𝑆ON algorithm is the correct answer.

Option (a) 𝑃CY algorithm is used for generating frequent itemsets by using candidate generation and pruning methods.

Option (c) Toivonen's algorithm is a sampling-based algorithm that uses a random sample of the data to approximate frequent itemsets with a specified error bound.

Option (d) Randomized algorithm is a broad term that refers to an algorithm that uses a random number generator to make decisions. It is not specific to frequent itemset mining.

86. Which one of the following is not a type of Contexts in Context-aware 
Computing? 
(a) Structural Context
(b) Mechanical Context
(c) Temporal Context
(d) User Context
(b) Mechanical Context is not a type of Contexts in Context-aware Computing.

Context-aware computing uses sensors, data analysis, and machine learning to detect and respond to the context of the user or the environment. There are various types of contexts in context-aware computing, including:

(a) Structural Context: It includes information about the physical structure of the environment, such as the location, size, and layout of objects.

(c) Temporal Context: It includes information about the time, such as the day of the week, the time of day, and the duration of an event.

(d) User Context: It includes information about the user, such as their preferences, interests, behavior, and social context.

Mechanical context does not fit into any of the above categories and is not recognized as a type of context in context-aware computing.

87. Which one of the following is not a cellular Architectural approach? 
(a) Cell Splitting
(b) Cell Breathing
(c) Cell Sectoring
(d) Reuse partitioning
The correct answer is (d) Reuse partitioning.

Cell Splitting, Cell Breathing, and Cell Sectoring are all cellular architectural approaches commonly used in cellular network design to improve network capacity, coverage, and quality of service.

Cell Splitting refers to dividing a single cell into multiple smaller cells, thereby increasing the number of cells in a given area and reducing the coverage area of each cell.

Cell Breathing involves dynamically adjusting the coverage area of a cell based on the number of users in the cell. During periods of low usage, the cell coverage area is expanded to cover a larger area, while during periods of high usage, the cell coverage area is contracted to cover a smaller area, thereby increasing the capacity of the cell.

Cell Sectoring involves dividing a cell into multiple sectors or directional antennas, each covering a smaller area. This helps to reduce interference and increase capacity in the cell.

Reuse Partitioning, on the other hand, refers to the frequency reuse pattern used in a cellular network, which is not a cellular architectural approach. It involves dividing the available frequency spectrum into a set of frequency channels, and reusing these channels across different cells to increase the network capacity.

88. Which of the following are Radio propagation mechanisms? 
1. Reflection and transmission
2. Scattering
3. Diffraction
 (a) 1 and 2 only
(b) 1, 2 and 3
(c) 1 and 3 only
(d) 2 and 3 only
All three options, reflection and transmission, scattering, and diffraction, are radio propagation mechanisms. Therefore, the correct answer is (b) 1, 2 and 3.

Reflection and transmission refer to the phenomenon where radio waves bounce off or pass through different materials, respectively. Scattering occurs when radio waves encounter small objects or irregularities in the propagation medium, causing the waves to scatter in various directions. Diffraction happens when radio waves bend around obstacles, such as buildings or hills, and spread out into the shadow zones behind them.

89. Which one of the following layers is not an Open System Interconnection 
(𝑂SI) layer? 
(a) Data link layer
(b) Transport layer
(c) Presentation layer
(d) Direction layer
The correct answer is (d) Direction layer.

The Open System Interconnection (OSI) model is a conceptual model that characterizes and standardizes the communication functions of a telecommunication or computing system. The OSI model is divided into seven layers, each of which is responsible for a specific aspect of network communication. These layers are:

Physical layer
Data link layer
Network layer
Transport layer
Session layer
Presentation layer
Application layer
There is no "Direction layer" in the OSI model, so (d) is the correct answer.

90. If the transmitter and receiver in a 𝑊LAN operating at 2.4 𝐺Hz are 
separated by a distance of 50 𝑚 , and the power transmitted by the 
transmitter is 10 𝑑Bm. The received power considering free-space 
propagation and Omni-directional antennas at both ends will be nearly: 
(a) −72 𝑑Bm
(b) −64 𝑑Bm
(c) −56 𝑑Bm
(d) −52 𝑑Bm
The received power considering free-space propagation can be calculated using the Friis transmission equation, which relates the transmitted power, the distance between the transmitter and receiver, and the gains of the antennas:

𝑃ᵣ = 𝑃ₜ + 𝐺ₜ + 𝐺ᵣ − 20 log₁₀(𝑑/𝑑₀)

where:
𝑃ᵣ is the received power
𝑃ₜ is the transmitted power (in dBm)
𝐺ₜ is the transmitter antenna gain
𝐺ᵣ is the receiver antenna gain
𝑑 is the distance between the transmitter and receiver
𝑑₀ is the reference distance, which is typically taken as 1 meter for free-space propagation

In this case, 𝑃ₜ = 10 dBm, 𝑑 = 50 m, 𝐺ₜ = 𝐺ᵣ = 0 (omni-directional antennas), and 𝑑₀ = 1 m. Substituting these values into the equation, we get:

𝑃ᵣ = 10 + 0 + 0 - 20 log₁₀(50/1) = −72 dBm

Therefore, the received power considering free-space propagation and omni-directional antennas at both ends will be approximately -72 dBm, which corresponds to option (a).

91. A NETCONF client that provides a command line interface for interacting 
with the Netopeer-server is:
 (a) Netopeer-manager 
(b) Netopeer-configurator 
(c) Netopeer-cli
(d) Netopeer-server
(c) Netopeer-cli is a NETCONF client that provides a command line interface for interacting with the Netopeer-server. Netopeer-manager is a graphical user interface (GUI) for managing NETCONF devices, while Netopeer-configurator is a tool for generating and managing NETCONF server configuration files. Netopeer-server is a NETCONF server implementation.

92. A protocol which handles sending e-mail and routing e-mail between mail 
servers is: 
 (a) Internet Protocol
(b) Transport control Protocol
(c) Simple mail transfer Protocol
(d) Server control Protocol
(c) Simple mail transfer Protocol (SMTP) handles sending and routing e-mail between mail servers. SMTP is an Internet standard protocol used for transmitting email messages over the internet. It is a text-based protocol, which means that the commands and data sent between mail servers are in plain text format. SMTP is responsible for transferring the message from the sender's mail server to the recipient's mail server.

93. A language which is used to model configuration and state data manipulated 
by the NETCONF protocol is: 
(a) Data Manipulation Language
(b) YANG data modeling language
(c) Shell Script Language
(d) Data Definition Language
(b) YANG data modeling language is the language used to model configuration and state data manipulated by the NETCONF (Network Configuration Protocol) protocol. YANG is a data modeling language used to model data for network management protocols such as NETCONF and RESTCONF. It provides a standardized way to describe data models, which allows devices and management systems from different vendors to interoperate more easily.

94. A networking architecture that separates the control plane from the data 
plane and centralizes the network controller is known as: 
(a) Software-Defined Networking
(b) Network-Function Virtualization
(c) Machine-to-Network
(d) Centralized Network Controller
The correct answer is (a) Software-Defined Networking.

Software-Defined Networking (SDN) is a networking architecture that separates the control plane from the data plane and centralizes the network controller. In SDN, the control plane is responsible for making decisions about how data packets should be forwarded, while the data plane is responsible for actually forwarding the packets. By separating the two planes and centralizing the network controller, SDN can provide greater flexibility, scalability, and programmability to network management and configuration.

Network-Function Virtualization (NFV) is another networking architecture that aims to virtualize network functions such as firewalls, routers, and load balancers, and run them on generic hardware.

Machine-to-Network (M2N) is a term used to describe communication between machines and the network infrastructure, usually in the context of the Internet of Things (IoT).

Centralized Network Controller is a term that refers to a network architecture where the network controller is located in a centralized location, as opposed to a distributed architecture where the controller functions are distributed across the network.

95. A template that allows separation of the presentation of data from the actual 
data by using placeholders and associated logic is: 
(a) Django template
(b) 𝑈RL template
(c) Xively cloud template
(d) Form template
(a) Django template.

Django template is a template engine used in Django web framework that allows the separation of presentation logic from the actual data by using placeholders and associated logic. It allows developers to design a layout or structure for their web application using HTML, CSS, and JavaScript and use placeholders to dynamically populate the content with data from a database or other sources. This separation of concerns helps in improving the maintainability, scalability, and reusability of web applications.

96. The time taken by the header of a message to travel between two directlyconnected nodes in the network is called:
 (a) Startup time
(b) Per-hop time
(c) Per-word transfer time
(d) Word-transfer time
The time taken by the header of a message to travel between two directly-connected nodes in the network is called per-hop time.

Per-hop time refers to the time taken by a packet to travel from one network node to the next network node in the transmission path. It includes the time taken to transmit the header information of the packet from one node to another.

Option (b) is the correct answer:
(b) Per-hop time.

97. Which one of the following decomposition techniques is used to decompose 
problems whose underlying computations correspond to a search of a space 
for solutions? 
 (a) Recursive decomposition
(b) Data decomposition
(c) Exploratory decomposition
(d) Speculative decomposition
(c) Exploratory decomposition is the decomposition technique used to decompose problems whose underlying computations correspond to a search of a space for solutions. Exploratory decomposition involves breaking down the problem into smaller subproblems that can be explored independently, with the goal of identifying a solution or a path towards a solution. This technique is commonly used in search algorithms such as depth-first search and breadth-first search.

Recursive decomposition involves breaking down a problem into smaller subproblems of the same type, and solving each subproblem recursively. Data decomposition involves breaking down a problem into smaller subproblems that can be solved independently and in parallel. Speculative decomposition involves breaking down a problem into smaller subproblems that can be solved independently, and then speculatively executing multiple solutions in parallel to identify the best one.

98. In which parallel computer, the same instruction is executed synchronously 
by all processing units? 
(a) 𝑆SID
(b) 𝑆IMD
(c) 𝑀ISD
(d) 𝑀IMD
(c) MISD (Multiple Instruction Single Data) parallel computers execute multiple instructions simultaneously on the same data stream. In contrast, SIMD (Single Instruction Multiple Data) parallel computers execute the same instruction simultaneously on multiple data streams.

Therefore, option (c) 𝑀ISD is the correct answer as it executes the same instruction synchronously by all processing units. In 𝑀ISD machines, multiple processing units execute the same instruction stream, but each unit operates on different data. Examples of 𝑀ISD architectures include the Illiac IV and the STARAN.

Option (a) 𝑆SID (Single Instruction Single Data) and option (b) 𝑆IMD (Single Instruction Multiple Data) are not correct because they execute one instruction at a time, and option (d) 𝑀IMD (Multiple Instruction Multiple Data) executes multiple instructions on multiple data streams.

99. The ratio of time taken by single processor system and that taken by a 
parallel processing system is called: 
 (a) Efficiency
(b) Speed up
(c) Throughput
(d) Turnaround time
The ratio of time taken by a single processor system and that taken by a parallel processing system is called speed up. Therefore, option (b) is the correct answer.

Efficiency refers to the degree to which a system, device, or process performs its intended function with minimal waste of effort or resources. Throughput refers to the amount of data or information that can be processed in a given time period. Turnaround time refers to the total time it takes to complete a task, including processing time and waiting time.

100. The hardware technique that detects and resolves hazards is called:
(a) Interlock
(b) Intralock
(c) Halt
(d) Inter unlock

The hardware technique that detects and resolves hazards is called Interlock.
